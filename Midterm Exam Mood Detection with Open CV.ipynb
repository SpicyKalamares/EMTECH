{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ece5258",
   "metadata": {},
   "source": [
    "# CPE018 Midterm Exam (1st Sem, A.Y. 2023-2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942fbc9f",
   "metadata": {},
   "source": [
    "Student Submission Details:\n",
    "\n",
    "- Name:\n",
    "  - Joshua Mico Flores\n",
    "  - Richmond Ryan Reyes\n",
    "- Section: CPE31S4\n",
    "- Schedule: Thurday 1:30PM - 4:30PM\n",
    "- Instructor: Engr. Verlyn V. Nojor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce5f8e",
   "metadata": {},
   "source": [
    "# Intended Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90462228",
   "metadata": {},
   "source": [
    "By the end of this activity, the student should be able to:\n",
    "\n",
    "- ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "- ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1875a1e",
   "metadata": {},
   "source": [
    "# Tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6424d",
   "metadata": {},
   "source": [
    "For this examination, you must create a mood detection program with an object-oriented programming approach (same as project CAMEO), it must detect mood changes through the use of algorithms/techniques/schemes learned in class, and from external sources.\n",
    "\n",
    "In this file, you have to include for each section of your solution your completion of the following:\n",
    "\n",
    "- Part 1: Face Detection: Once your face is detected using any algorithm, it must draw an ROI. The color for the ROI is your choice; however, it must detect for all faces in the frame and draw a corresponding ROI.\n",
    "- Part 2: Face Recognition: The detected face must then be recognized, using any of the provided tools in class, the ROIs must indicate whether it is your face or someone it doesn't recognize.\n",
    "- Part 3: Mood Detection: Use three different feature detection and matching techniques to determine three emotion: happy, sad and neutral. Two of the techniques must be learned from class, and 1 must be one you independently learned.\n",
    "\n",
    "Properly show through your notebook the output for each part of the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf81cd7",
   "metadata": {},
   "source": [
    "# Procedure and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17369cac",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- This is the section where you have to include all your answers to the items provided in the tasks section.\n",
    "- Tasks 1 and 2 contribute directly to ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "- Task 3 contributes directly to ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d31d66",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd6232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "import cv2\n",
    "\n",
    "def detect():\n",
    "    face_cascade = cv2.CascadeClassifier(r'C:\\Users\\reyes\\Downloads/haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(r'C:\\Users\\reyes\\Downloads/haarcascade_eye.xml')\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            colored_frame = frame[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray, 1.2, 5, 0, (40, 40))\n",
    "\n",
    "        cv2.imshow(\"camera\", frame)\n",
    "        if cv2.waitKey(int(1000 / 12)) & 0xff == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a117e418",
   "metadata": {},
   "source": [
    "# Task 2: Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8d04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "def read_images(path):\n",
    "    X, y = [], []\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        filepath, label = str(row['Image Path']), int(row['Label'])\n",
    "        im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        im = cv2.resize(im, (200, 200))\n",
    "        X.append(np.asarray(im, dtype=np.uint8))\n",
    "        y.append(label)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def face_rec():\n",
    "    names = {1: 'richmond', 2: 'mico'}\n",
    "    X, y = read_images('dataset.csv')\n",
    "\n",
    "    if not X:\n",
    "        print(\"No data found. Please check if the file paths in the CSV file are correct.\")\n",
    "        return\n",
    "\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "    model = cv2.face.FisherFaceRecognizer_create()\n",
    "    model.train(X, y)\n",
    "\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier(r'C:\\Users\\Mico\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "            # Extract Region of Interest (ROI)\n",
    "            roi = img[y:y + h, x:x + w]\n",
    "            roi = cv2.resize(roi, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            params = model.predict(gray_roi)\n",
    "            print(params)\n",
    "\n",
    "            cv2.putText(img, f\"{names.get(params[0], 'Unknown')}, {params[1]}\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition\", img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    face_rec()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc968ed",
   "metadata": {},
   "source": [
    "# Task 3: Mood Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f88abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facial-emotion-recognition in c:\\users\\reyes\\anaconda3\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: facenet-pytorch in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facial-emotion-recognition) (2.5.3)\n",
      "Requirement already satisfied: opencv-python>=3.4.2 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facial-emotion-recognition) (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facial-emotion-recognition) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facenet-pytorch->facial-emotion-recognition) (2.31.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facenet-pytorch->facial-emotion-recognition) (0.16.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from facenet-pytorch->facial-emotion-recognition) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from requests->facenet-pytorch->facial-emotion-recognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from requests->facenet-pytorch->facial-emotion-recognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from requests->facenet-pytorch->facial-emotion-recognition) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from requests->facenet-pytorch->facial-emotion-recognition) (2023.7.22)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torchvision->facenet-pytorch->facial-emotion-recognition) (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (2023.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\reyes\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchvision->facenet-pytorch->facial-emotion-recognition) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install facial-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eb4ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EmotionRecognition.__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 45\u001b[0m     detect()\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m():\n\u001b[0;32m     26\u001b[0m     face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mreyes\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads/haarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     emotion_detector \u001b[38;5;241m=\u001b[39m EmotionRecognition()\n\u001b[0;32m     29\u001b[0m     camera \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: EmotionRecognition.__init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "# Technique 1\n",
    "\n",
    "import cv2\n",
    "\n",
    "import cv2\n",
    "from facial_emotion_recognition import EmotionRecognition\n",
    "\n",
    "def detect_mood_from_face(frame, face_cascade, emotion_detector):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Detect emotion\n",
    "        emotion, confidence = emotion_detector.predict_emotion(roi_gray)\n",
    "\n",
    "        # Display the detected emotion on the frame\n",
    "        cv2.putText(frame, f\"Emotion: {emotion} ({confidence:.2f})\", (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def detect():\n",
    "    face_cascade = cv2.CascadeClassifier(r'C:\\Users\\reyes\\Downloads/haarcascade_frontalface_default.xml')\n",
    "    emotion_detector = EmotionRecognition()\n",
    "\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        # Perform facial expression detection\n",
    "        frame = detect_mood_from_face(frame, face_cascade, emotion_detector)\n",
    "\n",
    "        cv2.imshow(\"camera\", frame)\n",
    "        if cv2.waitKey(int(1000 / 12)) & 0xff == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e1717",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4381e",
   "metadata": {},
   "source": [
    "For the three different techniques you used in face detection, provide an in-depth analysis.\n",
    "\n",
    "To do this, you must:\n",
    "\n",
    "- Test the face detection, face recongition, and mood detection functions 10 times each. Only the mood detection will have components for 10 tests for each different technique used.\n",
    "- Create a table containing the 10 tests (like shown below) for each task.\n",
    "- Analyze each output by identifying the accuracy and providing your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module for tabulating the data\n",
    "from tabulate import tabulate\n",
    " \n",
    "# Create a list for content of the table\n",
    "test_results = [\n",
    "    [\"1\", \"Happy\", \"Sad\", 0],\n",
    "    [\"2\", \"Happy\", \"Neutral\", 0],\n",
    "    [\"3\", \"Happy\", \"Happy\", 1]\n",
    "]\n",
    " \n",
    "# Create a list for the headers of your table\n",
    "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
    " \n",
    "# display table\n",
    "print(\"Task 3A: Mood Detection using XYZ Algorithm\")\n",
    "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
    "\n",
    "# Calculate for the accuracy\n",
    "total = 0\n",
    "for i in test_results:\n",
    "    total += i[3]\n",
    "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba757ca",
   "metadata": {},
   "source": [
    "# Summary and Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c41a73",
   "metadata": {},
   "source": [
    "This section must be answered concisely. Do not engage in unmeaningful writing for the summary and lessons learned. Provide your brief reflection only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2006c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d28305",
   "metadata": {},
   "source": [
    "**Proprietary Clause**\n",
    "\n",
    "Property of the Technological Institute of the Philippines (T.I.P.). No part of the materials made and uploaded in this learning management system by T.I.P. may be copied, photographed, printed, reproduced, shared, transmitted, translated or reduced to any electronic medium or machine-readable form, in whole or in part, without prior consent of T.I.P.\n",
    "\n",
    "Prepared by Engr. RMR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
